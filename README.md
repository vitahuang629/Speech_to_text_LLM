# ğŸ™ï¸ Voice Record Assistant

## ğŸ¯ Purpose

This project is designed to assist **consultants**, **physical therapists**, and **physicians** in efficiently recording conversations with patients. It also helps automatically extract and structure key customer information during dialogues.

æ­¤å°ˆæ¡ˆæ—¨åœ¨å”åŠ©**è«®è©¢å¸«**ã€**ç‰©ç†æ²»ç™‚å¸«**èˆ‡**é†«å¸«**ï¼Œåœ¨èˆ‡ç—…äººå°è©±éç¨‹ä¸­æœ‰æ•ˆç´€éŒ„å°è©±å…§å®¹ï¼Œä¸¦è‡ªå‹•æ•´ç†ç—…æ‚£çš„å€‹äººè¼ªå»“èˆ‡é‡é»è³‡è¨Šã€‚

---

## ğŸ› ï¸ Methodology

### 1. Data Preparation & Model Training
- Audio recordings are segmented into 30-second clips.
- Each clip is manually corrected and aligned with transcripts.
- These audio-text pairs are used to fine-tune **Azure Speech-to-Text**, along with collecting domain-specific vocabulary.

> éŸ³æª”æœƒè¢«åˆ‡å‰²æˆæ¯æ®µ 30 ç§’ï¼Œä¸¦é€²è¡Œäººå·¥æ ¡æ­£å¾Œï¼Œèˆ‡æ–‡å­—é…å°ï¼Œä½œç‚ºè¨“ç·´èªéŸ³è¾¨è­˜æ¨¡å‹ï¼ˆAzure Speechï¼‰çš„è³‡æ–™ä¾†æºã€‚  
> åŒæ™‚æ”¶é›†ç›¸é—œå°ˆæ¥­è¡“èªï¼Œä»¥æå‡è¾¨è­˜æº–ç¢ºåº¦ã€‚

### 2. Summarization with LLM
- Workflow: The transcribed text is processed to generate structured and concise consultation summaries.
- Initial Approach: We initially deployed the Ollama Breeze 7B model on AWS EC2 for cost-effective, self-hosted inference.
- Optimization: To meet the high standards for accuracy required by the business, we transitioned the underlying model to OpenAI's API (GPT-4o/GPT-3.5), ensuring more precise extraction of key medical information.

> æµç¨‹èªªæ˜ï¼š å°‡èªéŸ³è½‰å¯«å¾Œçš„æ–‡æœ¬è¼¸å…¥èªè¨€æ¨¡å‹ï¼Œç”Ÿæˆçµæ§‹åŒ–ä¸”ç²¾ç°¡çš„çœ‹è¨ºæ‘˜è¦ã€‚
> åˆæœŸæ¶æ§‹ï¼š æœ€åˆæ¡ç”¨éƒ¨ç½²æ–¼ AWS EC2 çš„ Ollama Breeze 7B æ¨¡å‹ï¼Œé€²è¡Œæœ¬åœ°ç«¯æ¨è«–ä»¥æ¸¬è©¦å¯è¡Œæ€§ã€‚
> æ¶æ§‹å„ªåŒ–ï¼š è€ƒé‡åˆ°å•†æ¥­æ‡‰ç”¨å°æ–¼å…§å®¹æº–ç¢ºæ€§çš„é«˜æ¨™æº–è¦æ±‚ï¼Œç³»çµ±æœ€çµ‚é·ç§»è‡³ OpenAI APIï¼Œé¡¯è‘—æå‡äº†å°è©±é‡é»æ•æ‰èˆ‡ç—…æ­·è³‡è¨Šçš„ç²¾ç¢ºåº¦ã€‚

---

## ğŸ–¼ï¸ Demo Screenshot

![ç³»çµ±æ¶æ§‹åœ–](static/workflow.png)

---

## ğŸ™‹â€â™€ï¸ My Role

I was responsible for the **backend development**, including:

- Implementing the **WebSocket server** for real-time audio streaming  
- Integrating **Azure Speech-to-Text** for accurate voice transcription  
- Deploying and connecting a **self-hosted LLM (Ollama Breeze 7B)** on **AWS EC2** for summarization  
- Building a **frontend demo interface** to visualize the transcription and summary process

> æœ¬äººè² è²¬å¾Œç«¯ç³»çµ±æ¶æ§‹è¨­è¨ˆã€èªéŸ³è¾¨è­˜èˆ‡æ‘˜è¦ç”Ÿæˆæµç¨‹ä¸²æ¥ï¼Œä¸¦è£½ä½œå‰ç«¯ä»‹é¢ä½œç‚ºå±•ç¤ºã€‚

---

## ğŸ“Œ Applications

- Medical consultation record automation  
- Physiotherapy session summarization  
- Aesthetic and wellness clinic records

---


## å°ˆæ¡ˆçµæ§‹

- consult_ws.py WebSocket service for the consultant's side. Handles real-time audio streaming and speech recognition for consultants.
- doct_ws.py WebSocket service for the doctor's side. Handles real-time audio streaming and speech recognition for doctors.
- index.html Frontend testing interface. Used to simulate WebSocket audio transmission and display speech recognition results.
- Legacy / Backup Files (main1.py, main.py, index1.html) These are previous iterations or backup files kept for reference.

> consult_ws.py è«®è©¢å¸«ç”¨çš„ WebSocket æœå‹™ï¼Œè² è²¬æ¥æ”¶è«®è©¢å¸«ç«¯èªéŸ³ä¸¦è™•ç†è¾¨è­˜ã€‚
> doct_ws.py é†«ç”Ÿç”¨çš„ WebSocket æœå‹™ï¼Œè² è²¬æ¥æ”¶é†«ç”Ÿç«¯èªéŸ³ä¸¦è™•ç†è¾¨è­˜ã€‚
> index.html å‰ç«¯æ¸¬è©¦é é¢ï¼Œå¯ç”¨ä¾†æ¨¡æ“¬ WebSocket èªéŸ³å‚³è¼¸èˆ‡è¾¨è­˜çµæœã€‚
> å…¶ä»–æª”æ¡ˆ (Other Files) main1.py, main.py, index1.html æ˜¯ä¹‹å‰çš„ä¿®æ”¹æª”ã€‚

## ğŸ‘¨â€ğŸ’» Author

**Vita Huang**  
Backend & AI Integration  
[GitHub @vitahuang629](https://github.com/vitahuang629)

